{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a text into words, phrases, symbols, or other meaningful elements. The tokens can be words, sentences, or subwords. Tokenization is a crucial step in natural language processing (NLP) tasks such as text classification, named entity recognition, and machine translation. In this notebook, we will explore different tokenization techniques and libraries in Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Zahid is a passionate AI enthusiast, currently diving into the exciting field of generative AI. \n",
    "As a dedicated computer science student specializing in artificial intelligence, he is constantly exploring innovative \n",
    "technologies to solve real-world problems.\n",
    " His curiosity and drive fuel his ambition to contribute meaningfully to the rapidly evolving AI landscape.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zahid is a passionate AI enthusiast, currently diving into the exciting field of generative AI. \n",
      "As a dedicated computer science student specializing in artificial intelligence, he is constantly exploring innovative \n",
      "technologies to solve real-world problems.\n",
      " His curiosity and drive fuel his ambition to contribute meaningfully to the rapidly evolving AI landscape.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zahid is a passionate AI enthusiast, currently diving into the exciting field of generative AI.',\n",
       " 'As a dedicated computer science student specializing in artificial intelligence, he is constantly exploring innovative \\ntechnologies to solve real-world problems.',\n",
       " 'His curiosity and drive fuel his ambition to contribute meaningfully to the rapidly evolving AI landscape.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zahid is a passionate AI enthusiast, currently diving into the exciting field of generative AI.\n",
      "As a dedicated computer science student specializing in artificial intelligence, he is constantly exploring innovative \n",
      "technologies to solve real-world problems.\n",
      "His curiosity and drive fuel his ambition to contribute meaningfully to the rapidly evolving AI landscape.\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zahid',\n",
       " 'is',\n",
       " 'a',\n",
       " 'passionate',\n",
       " 'AI',\n",
       " 'enthusiast',\n",
       " ',',\n",
       " 'currently',\n",
       " 'diving',\n",
       " 'into',\n",
       " 'the',\n",
       " 'exciting',\n",
       " 'field',\n",
       " 'of',\n",
       " 'generative',\n",
       " 'AI',\n",
       " '.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'dedicated',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'student',\n",
       " 'specializing',\n",
       " 'in',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'he',\n",
       " 'is',\n",
       " 'constantly',\n",
       " 'exploring',\n",
       " 'innovative',\n",
       " 'technologies',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'real-world',\n",
       " 'problems',\n",
       " '.',\n",
       " 'His',\n",
       " 'curiosity',\n",
       " 'and',\n",
       " 'drive',\n",
       " 'fuel',\n",
       " 'his',\n",
       " 'ambition',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'meaningfully',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rapidly',\n",
       " 'evolving',\n",
       " 'AI',\n",
       " 'landscape',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zahid\n",
      "is\n",
      "a\n",
      "passionate\n",
      "AI\n",
      "enthusiast\n",
      ",\n",
      "currently\n",
      "diving\n",
      "into\n",
      "the\n",
      "exciting\n",
      "field\n",
      "of\n",
      "generative\n",
      "AI\n",
      ".\n",
      "As\n",
      "a\n",
      "dedicated\n",
      "computer\n",
      "science\n",
      "student\n",
      "specializing\n",
      "in\n",
      "artificial\n",
      "intelligence\n",
      ",\n",
      "he\n",
      "is\n",
      "constantly\n",
      "exploring\n",
      "innovative\n",
      "technologies\n",
      "to\n",
      "solve\n",
      "real-world\n",
      "problems\n",
      ".\n",
      "His\n",
      "curiosity\n",
      "and\n",
      "drive\n",
      "fuel\n",
      "his\n",
      "ambition\n",
      "to\n",
      "contribute\n",
      "meaningfully\n",
      "to\n",
      "the\n",
      "rapidly\n",
      "evolving\n",
      "AI\n",
      "landscape\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
